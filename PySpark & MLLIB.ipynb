{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark and Mllib\n",
    "## I.- find lowest rated movies with PySpark\n",
    "## II.- predict user's movie rating score using Mllib  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.- Finding lowest rated movies using PySpark\n",
    "this is just to spare our friends some time and make sure they don't waist time wathing these instead of boing out with family or walking the dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('C:\\\\Users\\\\asuhajda\\\\JupyterRoot\\\\BigData\\\\movies_dataset\\\\ml-100k\\\\ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LowestRatedMovieDataFrame.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "def parseInput(line):\n",
    "    fields = line.split()\n",
    "    return Row(movieID = int(fields[1]), rating = float(fields[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession in Windows \n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  movie ID\n",
    "movieNames = loadMovieNames()\n",
    "#movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data\n",
    "while True:\n",
    "     try:\n",
    "         lines = spark.sparkContext.textFile(\"u.data\")\n",
    "         break\n",
    "     except ValueError:\n",
    "         #lines = spark.sparkContext.textFile(\"u.data\")\n",
    "         print('uups, somethign went wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these are the first 10 records\n",
      "[Row(movieID=242, rating=3.0), Row(movieID=302, rating=3.0), Row(movieID=377, rating=1.0), Row(movieID=51, rating=2.0), Row(movieID=346, rating=1.0), Row(movieID=474, rating=4.0), Row(movieID=265, rating=2.0), Row(movieID=465, rating=5.0), Row(movieID=451, rating=3.0), Row(movieID=86, rating=3.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert  it to a RDD of Row objects with (movieID, rating)\n",
    "''' '''\n",
    "while True:\n",
    "     try:\n",
    "         movies = lines.map(parseInput)\n",
    "         break\n",
    "            \n",
    "     except ValueError:\n",
    "         print('uups, somethign went wrong')\n",
    "\n",
    "print(\"these are the first 10 records\"), print(movies.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these are the first 10 records\n",
      "[Row(movieID=242, rating=3.0), Row(movieID=302, rating=3.0), Row(movieID=377, rating=1.0), Row(movieID=51, rating=2.0), Row(movieID=346, rating=1.0), Row(movieID=474, rating=4.0), Row(movieID=265, rating=2.0), Row(movieID=465, rating=5.0), Row(movieID=451, rating=3.0), Row(movieID=86, rating=3.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "print(\"these are the first 10 records\"), print(movieDataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieID=474, avg(rating)=4.252577319587629),\n",
       " Row(movieID=29, avg(rating)=2.6666666666666665),\n",
       " Row(movieID=26, avg(rating)=3.452054794520548),\n",
       " Row(movieID=964, avg(rating)=3.3333333333333335),\n",
       " Row(movieID=1677, avg(rating)=3.0),\n",
       " Row(movieID=65, avg(rating)=3.5391304347826087),\n",
       " Row(movieID=191, avg(rating)=4.163043478260869),\n",
       " Row(movieID=1224, avg(rating)=2.6666666666666665),\n",
       " Row(movieID=558, avg(rating)=3.6714285714285713),\n",
       " Row(movieID=1010, avg(rating)=3.25)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average rating for each movieID\n",
    "averageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n",
    "averageRatings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count of ratings for each movieID\n",
    "counts = movieDataset.groupBy(\"movieID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two together (We now have movieID, avg(rating), and count columns)\n",
    "averagesAndCounts = counts.join(averageRatings, \"movieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieID=229, count=171, avg(rating)=3.111111111111111),\n",
       " Row(movieID=843, count=30, avg(rating)=3.0),\n",
       " Row(movieID=161, count=220, avg(rating)=3.481818181818182),\n",
       " Row(movieID=861, count=3, avg(rating)=2.3333333333333335),\n",
       " Row(movieID=499, count=62, avg(rating)=3.9516129032258065),\n",
       " Row(movieID=208, count=200, avg(rating)=3.945),\n",
       " Row(movieID=1475, count=8, avg(rating)=2.875),\n",
       " Row(movieID=870, count=9, avg(rating)=2.7777777777777777),\n",
       " Row(movieID=1023, count=31, avg(rating)=2.5806451612903225),\n",
       " Row(movieID=762, count=115, avg(rating)=3.391304347826087),\n",
       " Row(movieID=1538, count=3, avg(rating)=3.0),\n",
       " Row(movieID=1387, count=3, avg(rating)=2.0),\n",
       " Row(movieID=1261, count=4, avg(rating)=3.25),\n",
       " Row(movieID=1173, count=7, avg(rating)=2.5714285714285716),\n",
       " Row(movieID=173, count=324, avg(rating)=4.172839506172839),\n",
       " Row(movieID=569, count=67, avg(rating)=2.701492537313433)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample joined Dataset  (replacement, fraction and seed)\n",
    "averagesAndCounts.sample(False, 0.01, 15).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the top 10 results by average rating and store it in new variable \n",
    "topTen = averagesAndCounts.orderBy(\"avg(rating)\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostile Intentions (1994) 1 1.0\n",
      "Careful (1992) 1 1.0\n",
      "Lotto Land (1995) 1 1.0\n",
      "Low Life, The (1994) 1 1.0\n",
      "Falling in Love Again (1980) 2 1.0\n",
      "Power 98 (1995) 1 1.0\n",
      "Amityville: A New Generation (1993) 5 1.0\n",
      "Further Gesture, A (1996) 1 1.0\n",
      "Amityville: Dollhouse (1996) 3 1.0\n",
      "Touki Bouki (Journey of the Hyena) (1973) 1 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print them out, converting movie ID's to names as we go.\n",
    "for movie in topTen:\n",
    "    print (movieNames[movie[0]], movie[1], movie[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.- Using MLLIB for Movie Recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load up movie ID -> movie name dictionary\n",
    "''' \n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            #movieNames[int(fields[0])] = fields[1]#.decode('ascii', 'ignore' , encoding='utf-32')\n",
    "            movieNames = movieNames.decode('ascii', 'ignore', 'r',  encoding='utf-32')\n",
    "    return movieNames\n",
    "'''\n",
    "\n",
    "# Convert u.data lines into (userID, movieID, rating) rows\n",
    "def parseInput(line):\n",
    "    fields = line.value.split()\n",
    "    return Row(userID = int(fields[0]), movieID = int(fields[1]), rating = float(fields[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieNames = loadMovieNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BUD-N083.itron.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MovieRecs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1deb29d7748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='196\\t242\\t3\\t881250949'),\n",
       " Row(value='186\\t302\\t3\\t891717742'),\n",
       " Row(value='22\\t377\\t1\\t878887116'),\n",
       " Row(value='244\\t51\\t2\\t880606923'),\n",
       " Row(value='166\\t346\\t1\\t886397596')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the raw data\n",
    "lines = spark.read.text(\"u.data\").rdd\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load up our movie ID -> name dictionary\n",
    "movieNames = loadMovieNames()\n",
    "\n",
    "\n",
    "# Convert it to a RDD of Row objects with (userID, movieID, rating)\n",
    "ratingsRDD = lines.map(parseInput)\n",
    "\n",
    "# Convert to a DataFrame and cache it\n",
    "ratings = spark.createDataFrame(ratingsRDD).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userID=196, movieID=242, rating=3.0),\n",
       " Row(userID=186, movieID=302, rating=3.0),\n",
       " Row(userID=22, movieID=377, rating=1.0),\n",
       " Row(userID=244, movieID=51, rating=2.0),\n",
       " Row(userID=166, movieID=346, rating=1.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userID=196, movieID=242, rating=3.0),\n",
       " Row(userID=186, movieID=302, rating=3.0),\n",
       " Row(userID=22, movieID=377, rating=1.0),\n",
       " Row(userID=244, movieID=51, rating=2.0),\n",
       " Row(userID=166, movieID=346, rating=1.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ALS collaborative filtering model from the complete data set\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"movieID\", ratingCol=\"rating\")\n",
    "model = als.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Print out ratings from user 1:\\nprint(\"\\nRatings for user ID 1:\")\\nuserRatings = ratings.filter(\"userID = 1\")\\nfor rating in userRatings.collect():\\n    print(movieNames[rating[\\'movieID\\']], rating[\\'rating\\'])\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Print out ratings from user 1:\n",
    "print(\"\\nRatings for user ID 1:\")\n",
    "userRatings = ratings.filter(\"userID = 1\")\n",
    "for rating in userRatings.collect():\n",
    "    print(movieNames[rating['movieID']], rating['rating'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 recommendations:\n",
      "Sling Blade (1996) 5.278684139251709\n",
      "Pulp Fiction (1994) 5.004818916320801\n",
      "Swingers (1996) 4.968068599700928\n",
      "Chasing Amy (1997) 4.951290607452393\n",
      "Godfather, The (1972) 4.84442663192749\n",
      "Being There (1979) 4.835037708282471\n",
      "Hoop Dreams (1994) 4.8089165687561035\n",
      "Monty Python's Life of Brian (1979) 4.779086589813232\n",
      "Secrets & Lies (1996) 4.770893573760986\n",
      "Grosse Pointe Blank (1997) 4.769832134246826\n",
      "Nikita (La Femme Nikita) (1990) 4.7573323249816895\n",
      "Welcome to the Dollhouse (1995) 4.749566078186035\n",
      "Chasing Amy (1997) 4.723423480987549\n",
      "Clerks (1994) 4.709767818450928\n",
      "Blade Runner (1982) 4.707928657531738\n",
      "Donnie Brasco (1997) 4.704612731933594\n",
      "Wrong Trousers, The (1993) 4.691067695617676\n",
      "Dead Man Walking (1995) 4.677611351013184\n",
      "Boot, Das (1981) 4.664533615112305\n",
      "Usual Suspects, The (1995) 4.663516521453857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTop 20 recommendations:\")\n",
    "# Find movies rated more than 100 times\n",
    "ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count > 100\")\n",
    "# Construct a \"test\" dataframe for user 1 with every movie rated more than 100 times\n",
    "popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(1))\n",
    "\n",
    "# Run our model on that list of popular movies for user ID 0\n",
    "recommendations = model.transform(popularMovies)\n",
    "\n",
    "# Get the top 20 movies with the highest predicted rating for this user\n",
    "topRecommendations = recommendations.sort(recommendations.prediction.desc()).take(20)\n",
    "\n",
    "for recommendation in topRecommendations:\n",
    "    print (movieNames[recommendation['movieID']], recommendation['prediction'])\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
